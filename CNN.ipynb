{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(nn.Module):\n",
    "    def __init__(self, embeded_size):\n",
    "        #embeded vector is the final output of a embeded_size, which will be the input to RNN\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        #using a pretrained CNN architecture designed for image classification.\n",
    "        # pretrained=True : Loads pretrained weights from training on the ImageNet dataset.\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Prevents weights from being updated during training, \n",
    "        # as the pretrained ResNet will only be used for feature extraction.\n",
    "        for params in resnet.parameters():\n",
    "            params.requires_grad_(False)\n",
    "\n",
    "        modules = list(resnet.children())[:-1] #Removing the last layer which is the classification layer\n",
    "        # As we don't want to classify the data\n",
    "\n",
    "        # Wraps the remaining layers into a new sequential module.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        # Adding a embedding layer which is the final layer for our Encoder model\n",
    "        # this will return an embedded vector by Transforming the ResNet's output features into an embedding vector\n",
    "        # Resnet.fc.in_features = no.of features output by the resnet module, taken as input for embedded layer\n",
    "        self.embeded_layer = nn.Linear(resnet.fc.in_features, embeded_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Images passed through the resnet model which extracts the features/feature maps\n",
    "        features = self.resnet(images)\n",
    "        # flatenning those features\n",
    "        # Flattening: Converts the output into a 2D tensor of shape (batch_size, num_features)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        # creating embedding vector of those flatenned features\n",
    "        features = self.embeded_layer(features)\n",
    "\n",
    "        # output is the embedded vector\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Encoder on a Single Image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_path = \"/Users/laibaqureshi/Desktop/BAI project/000000000025.jpg\"\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "# Transformations on the image because we're using a pretrained resnet model\n",
    "# and we need to stick to those because the paper suggests that. \n",
    "# https://arxiv.org/pdf/1512.03385\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256), # Resizes the image's shortest side to 256 pixels.\n",
    "    transforms.CenterCrop(224), # Crops a 224Ã—224 region from the center.\n",
    "    transforms.ToTensor(), # Converts the image to a PyTorch tensor of shape (3, 224, 224) (channels, height, width).\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])    \n",
    "])\n",
    "\n",
    "image_tensor = preprocess(image).unsqueeze(0)\n",
    "# Unsqueeze: Adds a batch dimension, resulting in shape (1, 3, 224, 224)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laibaqureshi/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/laibaqureshi/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after 0: torch.Size([1, 64, 112, 112])\n",
      "Output after 1: torch.Size([1, 64, 112, 112])\n",
      "Output after 2: torch.Size([1, 64, 112, 112])\n",
      "Output after 3: torch.Size([1, 64, 56, 56])\n",
      "Output after 4: torch.Size([1, 256, 56, 56])\n",
      "Output after 5: torch.Size([1, 512, 28, 28])\n",
      "Output after 6: torch.Size([1, 1024, 14, 14])\n",
      "Output after 7: torch.Size([1, 2048, 7, 7])\n",
      "Output after 8: torch.Size([1, 2048, 1, 1])\n",
      "Final feature vector shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Encoder\n",
    "\n",
    "# Each image's embedded vector will have 256 values\n",
    "embed_size = 256\n",
    "encoder = CNN_Encoder(embed_size)\n",
    "\n",
    "# Sets the encoder to evaluation mode, disabling operations like dropout.\n",
    "encoder.eval()\n",
    "\n",
    "output = image_tensor\n",
    "for name, module in encoder.resnet.named_children():\n",
    "    output = module(output)\n",
    "    # printing the shape of the output after each ResNet layer.\n",
    "    print(f\"Output after {name}: {output.shape}\")\n",
    "    # Eg: Output after 0: torch.Size([1, 64, 112, 112])\n",
    "    # This means we get 64 feature maps of size 112 x 112\n",
    "\n",
    "\n",
    "\n",
    "# Get the final feature vector:\n",
    "output = output.view(output.size(0), -1)\n",
    "output = encoder.embeded_layer(output)\n",
    "\n",
    "print(f\"Final feature vector shape: {output.shape}\")\n",
    "#Final feature vector shape: torch.Size([1, 256]) -> 1 for 1 image and has 256 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector tensor([[-2.9099e-01, -5.2723e-01, -1.8377e-01, -5.3323e-01,  5.1252e-02,\n",
      "          6.1005e-02, -1.9923e-01,  4.7904e-02, -1.6908e-01,  3.0531e-01,\n",
      "          2.9983e-02,  4.2404e-01, -1.3624e-01, -5.7255e-02,  2.4101e-01,\n",
      "         -1.1460e-01,  1.8864e-01, -2.1262e-02,  3.1975e-01,  3.1031e-01,\n",
      "          2.5426e-02, -1.3570e-01,  6.8599e-01,  8.1763e-01, -3.1593e-01,\n",
      "          1.6124e-01,  1.3674e-01,  4.4296e-01,  2.7504e-01, -4.1518e-02,\n",
      "          1.3502e-01, -2.7734e-01, -9.9362e-02, -3.9854e-02,  1.3523e-02,\n",
      "         -1.1696e-04, -2.3635e-01, -4.4412e-01,  5.0014e-02, -6.0238e-01,\n",
      "          7.3476e-02,  2.9803e-01,  8.3499e-02, -1.4206e-01,  2.6459e-02,\n",
      "         -2.0448e-01, -3.0230e-01,  2.9800e-01,  6.8996e-01,  2.1789e-01,\n",
      "          4.3664e-01, -2.4174e-01,  3.0750e-01, -3.3873e-01,  3.0980e-01,\n",
      "          4.2058e-02,  3.4191e-01,  2.8138e-01,  6.4282e-02, -2.5940e-01,\n",
      "          5.4373e-01, -4.2265e-01,  5.6677e-01,  3.5769e-01, -3.6796e-01,\n",
      "          1.9611e-01,  5.2877e-02, -1.0054e-01,  3.4941e-01, -1.5636e-01,\n",
      "         -1.3611e-01,  3.9615e-01, -1.1824e-01,  8.1821e-02,  1.0637e-01,\n",
      "         -2.4841e-01,  2.3696e-02,  7.1617e-02,  5.8591e-01, -4.0303e-01,\n",
      "         -3.0677e-01, -1.0487e-01,  6.8789e-02,  1.9005e-01, -3.1782e-01,\n",
      "          3.9521e-01, -1.5476e-01, -7.8723e-01,  2.9059e-01, -7.4828e-03,\n",
      "          2.5765e-01,  2.6090e-01, -3.6158e-01,  2.1904e-01,  4.6186e-01,\n",
      "         -4.1897e-01,  3.4755e-01, -3.2500e-01, -7.1751e-01, -3.1421e-01,\n",
      "          3.0230e-01,  6.6908e-02,  3.3458e-01, -5.5622e-01, -3.9611e-01,\n",
      "         -6.2452e-01,  6.7185e-01,  2.5021e-01, -2.1079e-01,  2.7174e-01,\n",
      "          2.7108e-02, -5.3702e-02, -4.5407e-01,  3.8106e-01,  1.3465e-01,\n",
      "         -1.4449e-01, -4.1102e-01, -2.0190e-01, -4.5395e-02, -4.1312e-01,\n",
      "          1.2010e-01, -5.7933e-01, -1.6463e-01, -5.4330e-01,  1.1145e-01,\n",
      "         -4.8105e-01, -1.6859e-01,  2.9206e-01,  3.3953e-01,  3.1812e-01,\n",
      "         -1.2149e-01,  4.3244e-01,  2.3853e-01,  2.1701e-01, -2.5372e-01,\n",
      "         -3.2678e-01,  3.9773e-01,  1.5595e-01,  1.4272e-01,  2.1548e-01,\n",
      "         -1.0276e-01, -4.9499e-01, -2.4494e-02,  2.0422e-01,  7.1523e-01,\n",
      "          2.2390e-01, -2.8753e-01, -6.0904e-01,  2.8332e-01,  2.6210e-01,\n",
      "         -4.9373e-01, -1.4066e-01, -1.9180e-01,  1.5966e-01,  1.0081e-01,\n",
      "          5.6985e-01,  1.1863e-01, -1.2368e-01, -1.1673e-01,  2.7001e-01,\n",
      "          1.9802e-02,  6.1410e-01,  2.9079e-01,  2.3716e-01, -3.8775e-01,\n",
      "          2.8971e-01,  4.2991e-01, -2.9418e-01, -6.3014e-02, -1.0062e-01,\n",
      "          4.7148e-02,  1.3718e-01,  2.0863e-01, -4.1852e-01,  6.5631e-01,\n",
      "         -5.1119e-01, -1.0262e-01, -5.2425e-01, -5.6575e-03, -1.6382e-01,\n",
      "         -6.5260e-01, -3.2452e-01, -1.8148e-01,  8.8313e-02,  5.4467e-01,\n",
      "          1.0924e-01, -1.4236e-01, -1.0549e-02, -1.7333e-01,  1.3753e-01,\n",
      "          2.8765e-01,  4.6511e-01, -2.2570e-01,  1.0487e-01,  1.9600e-02,\n",
      "         -3.9683e-01, -2.5263e-02, -3.7130e-01, -3.6352e-01,  1.3925e-01,\n",
      "         -5.7184e-01, -1.5858e-01,  5.2948e-01, -1.3692e-01, -2.1673e-01,\n",
      "         -5.1122e-01, -1.4925e-01, -7.8091e-01,  3.9527e-01, -1.0815e-01,\n",
      "          3.9905e-01,  2.8695e-01,  2.3218e-01,  8.5546e-02,  3.1607e-01,\n",
      "         -3.1492e-01,  6.9997e-01,  3.5527e-01,  2.1964e-02,  2.5088e-01,\n",
      "          3.2551e-01, -3.5172e-03,  9.4857e-02, -5.2503e-01,  2.9633e-01,\n",
      "          5.0895e-01, -2.6430e-01, -7.7047e-01, -2.9574e-01,  5.3783e-01,\n",
      "         -2.2806e-01, -4.2537e-02, -7.4945e-02, -7.5255e-01,  2.3973e-01,\n",
      "         -4.2831e-02, -1.6556e-01, -2.7140e-01,  5.6857e-01, -2.0760e-01,\n",
      "          4.1929e-01,  5.0071e-01, -2.9862e-01, -3.1615e-01,  3.5480e-01,\n",
      "          2.3157e-01, -9.4885e-02, -1.2667e-01,  1.6947e-01, -5.7238e-02,\n",
      "          2.2542e-01, -2.0443e-01,  1.7227e-01,  4.0559e-01, -7.1782e-02,\n",
      "          4.1127e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding vector\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
